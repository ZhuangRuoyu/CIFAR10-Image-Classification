{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 710106050066476259\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15785944679\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5593524906556744719\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "import math\n",
    "import gc\n",
    "from sklearn.preprocessing import minmax_scale, StandardScaler, normalize\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Softmax, Reshape\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "\n",
    "# check whether the GPU is working\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "%matplotlib inline\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(99)\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(labels, num_class):\n",
    "    labels = np.eye(num_class)[labels.reshape(-1)]\n",
    "    return labels\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
    "\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    \n",
    "    images = np.transpose(images.reshape(len(images),3,32,32), (0, 2, 3, 1))\n",
    "    \n",
    "    if cls_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        \n",
    "        # Plot image.\n",
    "        ax.imshow(images[i, :, :, :], interpolation= 'nearest')\n",
    "        \n",
    "        cls_true_name = class_names[cls_true[i]]\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true_name)\n",
    "        else:\n",
    "            # Name of the predicted class.\n",
    "            cls_pred_name = class_names[cls_pred[i]]\n",
    "\n",
    "            xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Read in Data\n",
    "* scale the input X for train and validation data set\n",
    "* convert the original Y, which in labels format, into one hot encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_Y_labels), (test_X, test_Y_labels) = cifar10.load_data()\n",
    "\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "\n",
    "# Scale the input X for training and test data set\n",
    "train_X /= 255.0\n",
    "test_X /= 255.0\n",
    "\n",
    "# One hot encoding of Y\n",
    "train_Y = convert_to_one_hot(train_Y_labels, 10)\n",
    "test_Y = convert_to_one_hot(test_Y_labels, 10)\n",
    "\n",
    "# Class Names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# The shape of training X and test X\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Train a CNN in Keras\n",
    "* We used a 11 layers convolutional neural network, which purely made by convolutional layers, without max pooling and fully connnected layers.\n",
    "* We used the leaky ReLu instead of ReLu to boost the training speed\n",
    "* Overal, we train the CNN 150 epochs, and finally achieved more than 90% accuracy on validation set\n",
    "* It worth noting that, we have used the data augmentation which applied random fliping, shifting, rotating and zooming on the training set, to robust our model and generalize the performance on the validation set\n",
    "* We used a decayed learning rate during the 150 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "baseMapNum = 64\n",
    "img_size = 32\n",
    "channels = 3\n",
    "img_shape = (img_size, img_size, channels)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(baseMapNum, (3, 3), padding='same', input_shape = img_shape))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D( baseMapNum, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D( baseMapNum, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D( baseMapNum, (3, 3), padding='same', strides=2))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(2 * baseMapNum, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D(2 * baseMapNum, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D(2 * baseMapNum, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D(2 * baseMapNum, (3, 3),padding='same',  strides=2))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(2 * baseMapNum, (3, 3)))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D(2 * baseMapNum, (1, 1), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "model.add(Conv2D(num_classes, (1, 1), padding='same'))\n",
    "model.add(LeakyReLU(0.01))\n",
    "\n",
    "# model.add(Conv2D(num_classes, (6, 6), strides=4, padding='same'))#\n",
    "model.add(AveragePooling2D(6, 6))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "# model.summary()\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 0.2, \n",
    "    width_shift_range = 0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    zoom_range = 0.2, \n",
    "    horizontal_flip =True)\n",
    "\n",
    "# Train the model\n",
    "datagen.fit(train_X)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.SGD(lr = 0.01, decay = 1e-6, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_1  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 64),\n",
    "                    shuffle=True,\n",
    "                    epochs = 30, validation_data = (test_X, test_Y))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.005, decay = 1e-6, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_2  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 64),\n",
    "                    shuffle=True,\n",
    "                    epochs = 30, validation_data = (test_X, test_Y))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.003, decay = 1e-6, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_3  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 64),\n",
    "                    shuffle=True,\n",
    "                    epochs = 30, validation_data = (test_X, test_Y))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.001, decay = 1e-6, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_4  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 64),\n",
    "                    shuffle=True,\n",
    "                    epochs = 30, validation_data = (test_X, test_Y))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.0005, decay = 1e-6, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_5  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 64),\n",
    "                    shuffle=True,\n",
    "                    epochs = 15, validation_data = (test_X, test_Y))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.0001, decay = 1e-6, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_6  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 64),\n",
    "                    shuffle=True,\n",
    "                    epochs = 15, validation_data = (test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.00003, decay = 1e-4, momentum=0.9)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_info_7  = model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 16),\n",
    "                    shuffle=True,\n",
    "                    epochs = 30, validation_data = (test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the results together\n",
    "acc = model_info_1.history['acc'] + model_info_2.history['acc'] + model_info_3.history['acc'] + model_info_4.history['acc'] + model_info_5.history['acc'] + model_info_6.history['acc'] #+ model_info_7.history['acc']\n",
    "val_acc =  model_info_1.history['val_acc'] + model_info_2.history['val_acc'] + model_info_3.history['val_acc'] + model_info_4.history['val_acc'] + model_info_5.history['val_acc'] + model_info_6.history['val_acc'] #+ model_info_7.history['val_acc']\n",
    "loss = model_info_1.history['loss'] + model_info_2.history['loss'] + model_info_3.history['loss'] + model_info_4.history['loss'] + model_info_5.history['loss'] + model_info_6.history['loss'] #+ model_info_7.history['loss']\n",
    "val_loss = model_info_1.history['val_loss'] + model_info_2.history['val_loss'] + model_info_3.history['val_loss'] + model_info_4.history['val_loss'] + model_info_5.history['val_loss'] + model_info_6.history['val_loss'] #+ model_info_7.history['val_loss']\n",
    "\n",
    "len_acc = len(acc) \n",
    "len_val_acc = len(val_acc)\n",
    "len_loss = len(loss)\n",
    "len_val_loss = len(val_loss)\n",
    "\n",
    "# Visualize the reults\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy during training\n",
    "axs[0].plot(range(1, len_acc + 1), acc)\n",
    "axs[0].plot(range(1, len_val_acc + 1), val_acc)\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_xticks(np.arange(1, len_acc + 1), len_acc / 10)\n",
    "axs[0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "# Loss during training\n",
    "axs[1].plot(range(1, len_loss +1), loss)\n",
    "axs[1].plot(range(1, len_val_loss +1), val_loss)\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_xticks(np.arange(1, len_loss + 1), len_loss / 10)\n",
    "axs[1].legend(['train', 'val'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime\n",
    "runtime = 30*100/60\n",
    "print('Runtime (in mins): ', runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize, title):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.binary)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    title_font = {'family': 'serif', 'color': 'black', 'weight': 'bold', 'size': 15}\n",
    "    label_font = {'family': 'serif', 'color': 'black', 'weight': 'normal', 'size': 10}\n",
    "    plt.title(title, fontdict=title_font)\n",
    "    plt.ylabel('True label',fontdict=label_font)\n",
    "    plt.xlabel('Predicted label',fontdict=label_font)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    \n",
    "# labels\n",
    "\n",
    "result = model.predict(test_X)\n",
    "\n",
    "test_Y  = test_Y_labels\n",
    "pred_Y = np.argmax( result, axis = 1)\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_Y, pred_Y)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False, \n",
    "                      title='Confusion Matrix of CNN with data augmentation')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Confusion Matrix of CNN with data augmentation')\n",
    "\n",
    "print(classification_report(test_Y, pred_Y, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Save the model stucture and trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"COMP5318_CNN_90_deeper_no_fc.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the weights to h5\n",
    "model.save_weights(\"COMP5318_CNN_90_deeper_no_fc.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('COMP5318_CNN_90_deeper_no_fc.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(\"COMP5318_CNN_90_deeper_no_fc.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "#model.compile(optimizer='rmsprop',loss='mse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
